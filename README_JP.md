# HydraLLM

[English](README.md) | [ç®€ä½“ä¸­æ–‡](README_CN.md) | æ—¥æœ¬èª

```mermaid
flowchart LR
  C["ç¾åœ¨ãƒ¢ãƒ‡ãƒ«ã‚’å†è©¦è¡Œ"] -->|"å¤±æ•—"| D["æ¬¡ãƒ¢ãƒ‡ãƒ«ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"];
  D -->|"æ¬¡ã¸"| C;
  C -->|"æˆåŠŸ"| E["âœ… å¿œç­”ã‚’è¿”ã™"];
  D -->|"å°½ããŸ"| F["âŒ ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™"];

  classDef muted fill:#F8FAFC,stroke:#CBD5E1,color:#64748B,stroke-width:1px;
  classDef flow fill:#FFF7ED,stroke:#EA580C,color:#7C2D12,stroke-width:3px;
  classDef flow2 fill:#FEF3C7,stroke:#D97706,color:#78350F,stroke-width:3px;
  classDef ok fill:#DCFCE7,stroke:#16A34A,color:#14532D,stroke-width:1.5px;
  classDef bad fill:#FEE2E2,stroke:#DC2626,color:#7F1D1D,stroke-width:1.5px;
  class C flow;
  class D flow2;
  class E ok;
  class F bad;
```

HydraLLM ã¯ã€è‡ªå‹•å†è©¦è¡Œã¨ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å‚™ãˆãŸé«˜æ€§èƒ½ãª LLM API ãƒ—ãƒ­ã‚­ã‚·ã§ã™ã€‚OpenAI äº’æ›ã€Anthropic äº’æ›ã€AWS Bedrock ã«å¯¾å¿œã—ã¾ã™ã€‚

ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—ã™ã‚‹ã¨ã€HydraLLM ã¯ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’å†è©¦è¡Œã—ã€å¿…è¦ã«å¿œã˜ã¦è¨­å®šæ¸ˆã¿ã®æ¬¡ãƒ¢ãƒ‡ãƒ«ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚æˆåŠŸã™ã‚‹ã‹ã€ã™ã¹ã¦ã®å€™è£œã‚’è©¦ã—åˆ‡ã‚‹ã¾ã§å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™ã€‚

> [!TIP]
> è¨­å®šé …ç›®ã®å®Œå…¨ãªä¸€è¦§ã¨ä¾‹ã¯ [CONFIGURATION.md](CONFIGURATION.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## âœ¨ HydraLLM ãŒé¸ã°ã‚Œã‚‹ç†ç”±

- ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° / ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰å‘ã‘ã®è‡ªå‹•å†è©¦è¡Œ + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
- OpenAI äº’æ›ã€Anthropic äº’æ›ã€AWS Bedrock ã®ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œã€‚
- ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒ¼ãƒ³ãŒå¤‰ã‚ã£ã¦ã‚‚ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®å˜ä¸€ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§å®‰å®šé‹ç”¨ã€‚

## ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### Homebrewï¼ˆmacOS / Linuxï¼‰

```bash
brew install fang2hou/tap/hydrallm
```

### Go ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå…¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼‰

```bash
go install github.com/fang2hou/hydrallm@latest
```

### ãƒã‚¤ãƒŠãƒªã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå…¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼‰

[GitHub Releases](https://github.com/fang2hou/hydrallm/releases) ã‹ã‚‰å–å¾—ã§ãã¾ã™ã€‚

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆGLM Coding Planï¼‰

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å«ã¾ã‚Œã‚‹ GLM showcase ã§ã¯ã€1 ã¤ã®è¨­å®šã§ 2 ã¤ã®ãƒªã‚¹ãƒŠãƒ¼ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚

- OpenAI äº’æ› API: `http://127.0.0.1:8101`
- Anthropic äº’æ› API: `http://127.0.0.1:8102`

### 1) è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™

**macOS / Linux:**

```bash
mkdir -p ~/.config/hydrallm
curl -o ~/.config/hydrallm/config.toml \
  https://raw.githubusercontent.com/fang2hou/hydrallm/main/showcases/glm-coding-plan.toml
```

**Windowsï¼ˆPowerShellï¼‰:**

```powershell
New-Item -ItemType Directory -Force -Path "$env:USERPROFILE\.config\hydrallm"
Invoke-WebRequest -Uri "https://raw.githubusercontent.com/fang2hou/hydrallm/main/showcases/glm-coding-plan.toml" -OutFile "$env:USERPROFILE\.config\hydrallm\config.toml"
```

### 2) API ã‚­ãƒ¼ã‚’è¨­å®š

**macOS / Linux:**

```bash
export ZAI_API_KEY="your-api-key"
```

**Windowsï¼ˆPowerShellï¼‰:**

```powershell
$env:ZAI_API_KEY = "your-api-key"
```

### 3) ãƒ—ãƒ­ã‚­ã‚·ã‚’èµ·å‹•

```bash
hydrallm
```

### 4) ãƒªã‚¹ãƒŠãƒ¼ã‚’ç¢ºèª

<details>
<summary><b>OpenAI äº’æ› ãƒªã‚¹ãƒŠãƒ¼ï¼ˆ8101ï¼‰</b></summary>

```bash
curl http://127.0.0.1:8101/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "placeholder",
    "messages": [{"role": "user", "content": "Say hello"}]
  }'
```

</details>

<details>
<summary><b>Anthropic äº’æ› ãƒªã‚¹ãƒŠãƒ¼ï¼ˆ8102ï¼‰</b></summary>

```bash
curl http://127.0.0.1:8102/v1/messages \
  -H "Content-Type: application/json" \
  -d '{
    "model": "placeholder",
    "max_tokens": 64,
    "messages": [{"role": "user", "content": "Say hello"}]
  }'
```

</details>

> [!NOTE]
> HydraLLM ã¯ã€å„ãƒªã‚¹ãƒŠãƒ¼ã§è¨­å®šã—ãŸãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã® `model` ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚

## ğŸ” ã‚µãƒ¼ãƒ“ã‚¹é‹ç”¨ï¼ˆè‡ªå‹•èµ·å‹•ï¼‰

Homebrew services ã‚’ä½¿ã†ã¨è‡ªå‹•èµ·å‹•ã‚’è¨­å®šã§ãã¾ã™ã€‚

> [!NOTE]
> `brew services` ã‚’ä½¿ã†å ´åˆã¯ã€`api_key` ã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚ã‚·ã‚§ãƒ«ã®ç’°å¢ƒå¤‰æ•°ã ã‘ã«ä¾å­˜ã—ãªã„ã§ãã ã•ã„ã€‚

```bash
brew services start hydrallm
brew services info hydrallm
brew services restart hydrallm
brew services stop hydrallm
```

- macOS: `launchd`ï¼ˆãƒ­ã‚°ã‚¤ãƒ³å¾Œã«è‡ªå‹•èµ·å‹•ï¼‰
- Linux: `systemd`

## ğŸ› ï¸ CLI ã‚³ãƒãƒ³ãƒ‰

| ã‚³ãƒãƒ³ãƒ‰ | èª¬æ˜ |
|---|---|
| `hydrallm` | ã‚µãƒ¼ãƒãƒ¼èµ·å‹• |
| `hydrallm serve` | ãƒ—ãƒ­ã‚­ã‚·èµ·å‹• |
| `hydrallm edit` | `$EDITOR` ã§è¨­å®šã‚’ç·¨é›† |
| `hydrallm version` | ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º |
| `hydrallm --help` | ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º |

ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ•ãƒ©ã‚°ï¼š`--config /path/to/config.toml`ã€`--log-level info`

## ğŸ§¯ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

ã‚¯ã‚¤ãƒƒã‚¯è¨ºæ–­ï¼š

```bash
hydrallm --config /path/to/config.toml --log-level debug
brew services list | grep hydrallm
```

<details>
<summary><b>è¨­å®šæ¤œè¨¼ã«å¤±æ•—ï¼šå°‘ãªãã¨ã‚‚ 1 ã¤ã®ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™ï¼ˆconfig validation failed: at least one model must be configuredï¼‰</b></summary>

`[models.<id>]` ã«æœ€ä½ 1 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚

</details>

<details>
<summary><b>model "..."ï¼šprovider "..." ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆmodel "...": provider "..." not foundï¼‰</b></summary>

å„ãƒ¢ãƒ‡ãƒ«ã® `provider` ãŒ `[providers.<name>]` ã«å­˜åœ¨ã™ã‚‹ã‚­ãƒ¼ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚

</details>

<details>
<summary><b>listener "..."ï¼šãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’æ··åœ¨ã§ãã¾ã›ã‚“ï¼ˆlistener "...": mixed model types are not allowedï¼‰</b></summary>

1 ã¤ã® listener å†…ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã® API ã‚¿ã‚¤ãƒ—ï¼ˆ`openai` / `anthropic` / `bedrock`ï¼‰ã‚’æ··åœ¨ã§ãã¾ã›ã‚“ã€‚
ã‚¿ã‚¤ãƒ—ã”ã¨ã« listener ã‚’åˆ†ã‘ã¦è¨­å®šã—ã¦ãã ã•ã„ã€‚

</details>

<details>
<summary><b>ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒä¸Šæµã§ 4xx/5xx ã‚’è¿”ã™ï¼ˆRequests return upstream 4xx/5xxï¼‰</b></summary>

`log.include_error_body = true` ã‚’ä¸€æ™‚çš„ã«æœ‰åŠ¹åŒ–ã™ã‚‹ã¨ã€ä¸Šæµã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ç¢ºèªã§ãã¾ã™ã€‚

</details>

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT
